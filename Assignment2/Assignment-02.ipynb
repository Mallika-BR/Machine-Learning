{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c7ef20",
   "metadata": {},
   "source": [
    "# Programming Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please make sure you are using a python3.9.x interpreter \n",
    "# and install the packages from requirements.txt (provided)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb583d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some basic imports. You can import any package at any cell\n",
    "from sklearn.preprocessing import OneHotEncoder   #My favorite categorical to numerical feature conversion tool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.linalg import inv\n",
    "from time import perf_counter\n",
    "import math\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97bf30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's load the given dataset\n",
    "# First load the dataset into pandas dataframe\n",
    "full_dataset = pd.read_csv('dataset/baby-weights-dataset.csv',delimiter=',')\n",
    "judge_dataset = pd.read_csv('dataset/judge-without-labels.csv',delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa22fe4",
   "metadata": {},
   "source": [
    "TASK 1:\n",
    "Separate the full_dataset into two parts: X and y, where X denotes the input matrix containing only the input (i.e., independent explanatory) variables, and y denotes the target variable containing only the target values for exactly the same number of samples in the given full_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb189f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101400, 36)\n",
      "(101400,)\n"
     ]
    }
   ],
   "source": [
    "#Make sure you write your solution to this task below in this cell only.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "X = full_dataset.iloc[:,:-1]\n",
    "y = full_dataset.iloc[:,-1]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a69687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let me test your code above\n",
    "assert X.shape==(101400,36)\n",
    "assert y.shape==(101400,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3692ee8",
   "metadata": {},
   "source": [
    "TASK 2:\n",
    "Given X representing the input matrix from the full_dataset, y being the target vector (the rightmost column of the full_dataset), obtained from Task 1:\n",
    "randomly split the (X,y) dataset into 75% for training and 25% for testing using the library function from the library sklearn.model_selection . Please pass to the train_test_split function an additional argument random_state=45931.\n",
    "Store the 4 splits as X_train, X_test, y_train, y_test respectively.\n",
    "Save the ID column for X_train and X_test into ID_train and ID_test as list variable.\n",
    "Now, drop the ID columns from both X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1018ccea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76050, 35) (25350, 35) (76050,) (25350,)\n"
     ]
    }
   ],
   "source": [
    "#Make sure you write your solution to this task below in this cell only.\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=45931)\n",
    "\n",
    "ID_train = X_train[\"ID\"]\n",
    "ID_test = X_test[\"ID\"]\n",
    "\n",
    "X_train = X_train.drop(\"ID\", axis=1)\n",
    "X_test = X_test.drop(\"ID\", axis=1)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb41492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let me test your code above\n",
    "assert (X_train.shape, X_test.shape, y_train.shape, y_test.shape) == ((76050, 35), (25350, 35), (76050,), (25350,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea8931",
   "metadata": {},
   "source": [
    "TASK 3:\n",
    "Compute mean, stdev, min, max, 25% percentile, median and 75% percentile of BWEIGHT target variable (i.e, the target y) in the training set (i.e., y_train), and print the computed values as a numpy array containing these 7 results (respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb3291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.256997863247864 1.3300496889691478 0.3125 13.0625 6.625 7.375 8.0625\n"
     ]
    }
   ],
   "source": [
    "#Make sure you write your solution to this task below in this cell only.\n",
    "mean_val = 0\n",
    "stdev_val = 0\n",
    "min_val = 0\n",
    "max_val = 0\n",
    "percentile25_val = 0\n",
    "median_val = 0\n",
    "percentile75_val = 0\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "mean_val = np.mean(y_train)\n",
    "stdev_val = np.std(y_train)\n",
    "min_val = np.min(y_train)\n",
    "max_val = np.max(y_train)\n",
    "percentile25_val = np.percentile(y_train, 25)\n",
    "median_val = np.percentile(y_train, 50)\n",
    "percentile75_val = np.percentile(y_train, 75)\n",
    "\n",
    "print(mean_val,stdev_val,min_val,max_val,percentile25_val,median_val,percentile75_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ded19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(sum([mean_val,stdev_val, min_val,max_val, percentile25_val,median_val,percentile75_val])-44.024547552217015) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de8efd",
   "metadata": {},
   "source": [
    "TASK 4:\n",
    "A little background first: Categorical features are features that contain values that are not numeric. As you can imagine non-numeric values will create trouble (by introducing nan) during calculation to gradients, and whatnot, right? The maths are undefined when these get in its way. An obvious solution you may be intrigued to do is dropping the features! Aha! Wrong!! Every piece of data is precious... as those non-numeric features may present with valuable insights of the data samples to find the patterns to map inputs with output/targets. So, we should include them. But, how?\n",
    "\n",
    "The answer is via \"Encoding\" we can make good use the non-numeric variables.\n",
    "\n",
    "There are several types of encoding used in practice. Here are the two popular ones:\n",
    "\n",
    "Label Encoding, where labels are encoded as subsequent numbers. Say, for a categorical feature named \"Category\" with three categorical values: {“Cat”, “Dog” or “Zebra”} can be encoded to \"0\", \"1\", \"2\" respectively as in figure below. The issue with this type of encoding may unintentionally impose a type of ordering of the categories, that may add bias to the training.label-encoding\n",
    "One Hot Encoding, ignores the ordering of the categories all together. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1. Also, don't forget to remove the original categorical features. Here below just an example, how to convert the categorical feature called \"Category\" having the {“Cat”, “Dog” or “Zebra”} values into three new binary features: \"Cat\", \"Dog\", \"Zebra\".label-encoding\n",
    "A note on the Dummy Variable Trap The Dummy Variable Trap occurs when two or more dummy variables created by one-hot encoding are highly correlated (i.e., becomes multi-collinear). This means that one variable can be predicted from the others, making it difficult to interpret predicted coefficient variables in regression models. In other words, the individual effect of the dummy variables on the prediction model can not be interpreted well because of multicollinearity.\n",
    "\n",
    "Using the one-hot encoding method, a new dummy variable is created for each categorical variable to represent the presence (1) or absence (0) of the categorical variable. For example, if tree species is a categorical variable made up of the values pine, or oak, then tree species can be represented as a dummy variable by converting each variable to a one-hot vector. This means that a separate column is obtained for each category, where the first column represents if the tree is pine and the second column represents if the tree is oak. Each column will contain a 0 or 1 if the tree in question is of the column's species. These two columns are multi-collinear since if a tree is pine, then we know it's not oak and vice versa. The machine learning models trained on dataset having this multi-collinearity suffers. A remedy is to drop first (or any one) of the dummy (i.e., one-hot) features created.\n",
    "\n",
    "Given the training dataset (X_train, y_train), save as X_train_ohe after replacing all the non-numeric variables (i.e., categorical variables) with numeric encoding. Also, you need to use the same encoding to work on the X_train dataset as well and save it as X_test_oh. We need to make sure any new categorical value in the test dataset get ignored, i.e, when an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. Please complete the following almost complete function lets_do_one_hot_encoding() that encodes dataset on the supplied categorical feature column list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ade60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure you write your solution to this task below in the marked location in this cell only.\n",
    "\n",
    "\n",
    "def lets_do_one_hot_encoding(data, categorical_features, transform_only=True, encoders=[], verbose=False):\n",
    "    '''\n",
    "    The function does one_hot_encoding on the given dataset... \n",
    "    It's intentionally defined fully, meaning you do not need to do anything here... but show us\n",
    "    how to use it properly.\n",
    "    \n",
    "    Input: \n",
    "        * data -- it's pretty much either X_train, or X_test that you prepared previously, that is\n",
    "                  any dataframe having independent variables.\n",
    "        * categorical_features -- a list of column/feature names in the data (dataframe) that you think\n",
    "                  are non-numerical / i.e., categorical that you want to be converted to numerical\n",
    "                  using the one-hot encoding technique.\n",
    "        * transform_only -- a boolean parameter, if set to False, will learn (i.e., fit) various categorical\n",
    "                values in the categorical_features from the given dataset, and use this to encode the dataset\n",
    "                using one-hot encoding. This is important that you set to False on training dataset, and\n",
    "                True on test set. If new categorical values are present in the test dataset, those will be\n",
    "                ignored, making it easier to have same set of encoded features both in training and test \n",
    "                dataset... otherwise, subsequent operations (may/) will not work.\n",
    "        * encoders -- a list of one-hot encoders previously saved, and now will be used to encode given dataset.\n",
    "                If transform_only=True, the function looks for this provided list of encoders to encode the \n",
    "                dataset instead of learning new categories. Once again, it's expected that you pass the set\n",
    "                of encoders for each of the categorical features that you encoded the training set -- i.e.,\n",
    "                leave it empty for training set, and pass the set of encoders while encoding test set.\n",
    "    Returns:\n",
    "        * data -- the converted dataframe after the encoding is completed.\n",
    "        * enc_list -- is the list of encoders used to encode the given data.\n",
    "        * categorical_features -- is the list of categorical features that you would want to encode.\n",
    "        It's expected that for training dataset, you save the encoder list and the list of features for later\n",
    "        use to encode a test dataset.\n",
    "    '''\n",
    "    if len(encoders)>0:\n",
    "        enc_list=encoders\n",
    "    else:\n",
    "        enc_list = []\n",
    "    col_onehot = []\n",
    "    i = 0\n",
    "    for feature in categorical_features:\n",
    "        if verbose: print('Dealing with feature ={}'.format(feature))\n",
    "        if transform_only==True: #for test dataset\n",
    "            enc = enc_list[i]\n",
    "            c_onehot = enc.transform(data[[feature]])\n",
    "            i = i + 1\n",
    "        else: #fit and transform\n",
    "            enc = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n",
    "            c_onehot = enc.fit_transform(data[[feature]])\n",
    "            if verbose: print('c_onehot = {}'.format(c_onehot))\n",
    "            enc_list.append(enc)\n",
    "        c_onehot = pd.DataFrame(c_onehot, columns=list(enc.categories_[0][1:])) #dropped first column\n",
    "        if verbose: print('dropped first column...c_onehot = {}'.format(c_onehot))\n",
    "        c_onehot = c_onehot.add_prefix(feature+'_')\n",
    "        col_onehot.append(c_onehot)\n",
    "        if verbose: print('col_onehot = {}'.format(col_onehot))\n",
    "    \n",
    "    #concat all onehot feature columns\n",
    "    concat_df = pd.concat(col_onehot,axis=1)\n",
    "    #match index with given data\n",
    "    concat_df.index = data.index\n",
    "    \n",
    "    #Here below is your task:\n",
    "    #i) drop the categorical feature columns from dataframe `data`\n",
    "    #ii) and then merge with those new onehot features stored in `cocat_df`\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    data = data.drop(categorical_features, axis=1)\n",
    "    data = data.join(concat_df)\n",
    "    \n",
    "    return data,enc_list,categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4debfaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "let's apply one-hot encoding on two columns: \"HISPMOM\" and \"HISPDAD\" of the training dataset, X_train.\n",
    "Please call the function, lets_do_one_hot_encoding() you helped defining above passing appropriate arguments.\n",
    "Also, receive the 3 return values from the function call: X_train_ohe, enc_list and categorical_features to \n",
    "use them in the next question, that is, encoding the test dataset, X_test, and \n",
    "later in Task 17 to encode the judge set. \n",
    "\n",
    "It's always a good idea to save the list of encoders and features in a file (e.g., joblib), if you would \n",
    "want to apply your model to evaluate/predict on a new test sample. In a summary: if your model was trained with\n",
    "transformed one-hot encoded dataset, a new test data also needs to be encoded with the same list of variables\n",
    "defined by the encoding.\n",
    "\n",
    "'''\n",
    "X_train_ohe = []  #one-hot encoded training dataset\n",
    "enc_list = [] #encoder list to be created during one-hot encoding of the training dataset\n",
    "categorical_features = ['HISPMOM', 'HISPDAD'] #The list of categorical features in question.\n",
    "\n",
    "#Here below is your task:one-hot encoding of train set: X_train\n",
    "# YOUR CODE HERE\n",
    "X_train_ohe, enc_list, categorical_features = lets_do_one_hot_encoding(X_train, categorical_features, transform_only=False, encoders=[] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e4f5df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same number of columns i.e 45\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "let's apply one-hot encoding on the same two columns: \"HISPMOM\" and \"HISPDAD\" of the test dataset, X_test.\n",
    "Please call the function, lets_do_one_hot_encoding() you helped defining above with appropriate parameters.\n",
    "Be sure to use the encoders from the training set and turn on the `transform_only` switch to true.\n",
    "Also, receive the 3 return values from the function call in X_test_ohe, enc_list, categorical_features. \n",
    "Although, the second and third returned values can be ignored.\n",
    "\n",
    "Please make sure, the one-hot encoded training set and test set has the exact same number of features/columns,\n",
    "and in the same order. If not, the test in the next cell will almost certainly fail, and you will lose points,\n",
    "and the training and testing of the machine learning model will absolutely fail. So, please take close\n",
    "attention.\n",
    "\n",
    "'''\n",
    "X_test_ohe = []  #one-hot encoded test dataset\n",
    "\n",
    "#Here below is your task: one-hot encoding of test set: X_test\n",
    "# YOUR CODE HERE\n",
    "X_test_ohe, enc_list, categorical_features = lets_do_one_hot_encoding(X_test ,categorical_features, transform_only=True, encoders=enc_list)\n",
    "\n",
    "if(len(X_train_ohe.columns) == len(X_test_ohe.columns)):\n",
    "    print(\"Same number of columns i.e\",+len(X_train_ohe.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "668a1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X_train_ohe.columns)== len(X_test_ohe.columns) and len(X_train_ohe.columns) == sum([1 for i, j in zip(X_train_ohe.columns, X_test_ohe.columns) if i == j])\n",
    "assert (X_train_ohe.shape, X_test_ohe.shape)==((76050, 45), (25350, 45))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8457424c",
   "metadata": {},
   "source": [
    "TASK 5:\n",
    "Given the X_train_ohe (Onehot encoded Pandas Dataframe from Task 4), check if there are missing values, and if yes, count how many, and impute the missing values with corresponding mean values.\n",
    "Finally, print the counting result as a Pandas dataframe named \"missing_counts\" having 2 columns {variable_name,num_of_missing_values). Please make sure that the result lists all the input variables in the given dataset.\n",
    "Now, impute the missing values by mean of the respective variable and save the revised dataframe as X_train_ohe_imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc24e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable_name  num_of_missing_values\n",
      "SEX                                0\n",
      "MARITAL                            0\n",
      "FAGE                               0\n",
      "GAINED                             1\n",
      "VISITS                             0\n",
      "MAGE                               0\n",
      "FEDUC                              1\n",
      "MEDUC                              0\n",
      "TOTALP                             0\n",
      "BDEAD                              0\n",
      "TERMS                              0\n",
      "LOUTCOME                           0\n",
      "WEEKS                              1\n",
      "RACEMOM                            0\n",
      "RACEDAD                            0\n",
      "CIGNUM                             1\n",
      "DRINKNUM                           0\n",
      "ANEMIA                             0\n",
      "CARDIAC                            0\n",
      "ACLUNG                             0\n",
      "DIABETES                           0\n",
      "HERPES                             0\n",
      "HYDRAM                             1\n",
      "HEMOGLOB                           0\n",
      "HYPERCH                            0\n",
      "HYPERPR                            0\n",
      "ECLAMP                             0\n",
      "CERVIX                             0\n",
      "PINFANT                            0\n",
      "PRETERM                            0\n",
      "RENAL                              0\n",
      "RHSEN                              0\n",
      "UTERINE                            0\n",
      "HISPMOM_M                          0\n",
      "HISPMOM_N                          0\n",
      "HISPMOM_O                          0\n",
      "HISPMOM_P                          0\n",
      "HISPMOM_S                          0\n",
      "HISPMOM_U                          0\n",
      "HISPDAD_M                          0\n",
      "HISPDAD_N                          0\n",
      "HISPDAD_O                          0\n",
      "HISPDAD_P                          0\n",
      "HISPDAD_S                          0\n",
      "HISPDAD_U                          0\n",
      "Index(['GAINED', 'FEDUC', 'WEEKS', 'CIGNUM', 'HYDRAM'], dtype='object') 5\n",
      "After imputing the missing values\n",
      "Index([], dtype='object') 0\n"
     ]
    }
   ],
   "source": [
    "missing_counts = pd.DataFrame()\n",
    "X_train_ohe_imputed = X_train_ohe\n",
    "\n",
    "#Your task below. \n",
    "# i) Please prepare the missing_counts dataframe accordingly.\n",
    "# ii) impute missing value in a variable with corresponding mean of the variable.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "missing_counts = pd.DataFrame(X_train_ohe.isnull().sum()).rename_axis(\"variable_name\", axis = 1)\n",
    "missing_counts.rename(columns = {0: \"num_of_missing_values\"}, inplace = True)\n",
    "\n",
    "print(missing_counts)\n",
    "print(X_train_ohe.columns[X_train_ohe.isnull().any()], X_train_ohe.isnull().any(axis = 1).sum())\n",
    "\n",
    "\n",
    "X_train_ohe_imputed = X_train_ohe.fillna(X_train_ohe.mean())\n",
    "print(\"After imputing the missing values\")\n",
    "print(X_train_ohe_imputed.columns[X_train_ohe_imputed.isnull().any()], X_train_ohe_imputed.isnull().any(axis = 1).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50562d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(missing_counts['num_of_missing_values'])==5\n",
    "assert np.isnan(X_train_ohe.loc[[1748]][['FEDUC']].iloc[0,0])==True\n",
    "assert np.isnan(X_train_ohe_imputed.loc[[1748]][['FEDUC']].iloc[0,0])==False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62796517",
   "metadata": {},
   "source": [
    "TASK 6:\n",
    "Given a X_train_ohe_imputed (Pandas dataframe from Task 5) where all the categorical variables are already replaced with numeric values, print a list of top 20 highly correlated variables with respect to the target variable, and save the result as a Pandas dataframe named top20_df with 2 columns {variable,corr_score}.\n",
    "Here, the corr_score between a variable x and the target variable y needs to be computed using the Pearson Correlation Coefficient (PCC). Please note, PCC ranges between -1 to +1. PCC score 0 means no correlation, while value towards +1 and -1 represent positive and negative correlations respectively. For instance, PCC=0.8 and PCC=-0.8 tell similar strength positive and negative correlations between the two subject variables.\n",
    "Please do not include BWEIGHT in the top20_df list of top 20 correlated variable list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e392157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   variable  Corr Score\n",
      "12    WEEKS    0.565254\n",
      "3    GAINED    0.174466\n",
      "4    VISITS    0.129247\n",
      "25  HYPERPR   -0.111375\n",
      "1   MARITAL   -0.106297\n",
      "0       SEX   -0.091260\n",
      "15   CIGNUM   -0.086711\n",
      "14  RACEDAD   -0.084284\n",
      "13  RACEMOM   -0.078811\n",
      "29  PRETERM   -0.075237\n",
      "27   CERVIX   -0.068751\n",
      "5      MAGE    0.068550\n",
      "28  PINFANT    0.065963\n",
      "26   ECLAMP   -0.065429\n",
      "7     MEDUC    0.055249\n",
      "2      FAGE    0.051981\n",
      "6     FEDUC    0.051537\n",
      "22   HYDRAM   -0.050019\n",
      "24  HYPERCH   -0.047094\n",
      "32  UTERINE   -0.040339\n",
      "(20, 2)\n"
     ]
    }
   ],
   "source": [
    "top20_df = pd.DataFrame() #You need to save the top-20 most correlated variables with respect to BWEIGHT target\n",
    "\n",
    "# Your task below is:\n",
    "# i) on the imputed dataset you got from task 5, find the top-20 most-correlated variables\n",
    "# with respect to the target variable.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "corr_score = X_train_ohe_imputed.corrwith(y_train, axis=0, drop=True, method='pearson')\n",
    "\n",
    "\n",
    "corr_score = corr_score.rename_axis(\"variable\").reset_index()\n",
    "corr_score.rename(columns={ corr_score.columns[1]: \"Corr Score\" }, inplace = True)\n",
    "\n",
    "\n",
    "top20_df = corr_score.sort_values(by = \"Corr Score\", ascending = False, key = abs).iloc[:20]\n",
    "print(top20_df)\n",
    "print(top20_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157b6d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert top20_df.shape==(20,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d3171",
   "metadata": {},
   "source": [
    "TASK 7:\n",
    "Given the X_train_ohe_imputed (as Pandas dataframe from task 5) and and top20_df (as Pandas Dataframe from Task 6) having 2 columns {variable_name,corr_score} similar to the one you computed in Task 6:\n",
    "\n",
    "Please save as X_train_t20 keeping only the columns listed in the top20_df dataframe.\n",
    "Repeat the process for X_test_ohe (obtained from task 4), and save it as X_test_t20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98964154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76050, 20)\n",
      "Index([], dtype='object') 0\n",
      "   variable  Corr Score\n",
      "12    WEEKS    0.565732\n",
      "3    GAINED    0.169653\n",
      "4    VISITS    0.130641\n",
      "1   MARITAL   -0.109535\n",
      "15   CIGNUM   -0.102883\n",
      "0       SEX   -0.099990\n",
      "25  HYPERPR   -0.099812\n",
      "14  RACEDAD   -0.083358\n",
      "13  RACEMOM   -0.083194\n",
      "26   ECLAMP   -0.075661\n",
      "28  PINFANT    0.070742\n",
      "5      MAGE    0.068241\n",
      "29  PRETERM   -0.066039\n",
      "27   CERVIX   -0.061248\n",
      "7     MEDUC    0.057872\n",
      "6     FEDUC    0.056074\n",
      "2      FAGE    0.049824\n",
      "22   HYDRAM   -0.049789\n",
      "32  UTERINE   -0.049738\n",
      "9     BDEAD   -0.044062\n",
      "(25350, 20)\n"
     ]
    }
   ],
   "source": [
    "#Your task below is to slice training and test dataset to retain only the top-20 most-correlated variables\n",
    "# with respect to the target variable\n",
    "\n",
    "X_train_t20 = pd.DataFrame()\n",
    "X_test_t20 = pd.DataFrame()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "column_list = [i for i in top20_df[\"variable\"].values]\n",
    "X_train_t20 = X_train_ohe_imputed.loc[ :,column_list]\n",
    "print(X_train_t20.shape)\n",
    "\n",
    "# process for X_test_ohe\n",
    "\n",
    "missing_values = pd.DataFrame(X_test_ohe.isnull().sum()).rename_axis(\"variable_name\", axis = 1)\n",
    "missing_values.rename(columns = {0: \"num_of_missing_values\"}, inplace = True)\n",
    "#print(missing_values)\n",
    "print(X_test_ohe.columns[X_test_ohe.isnull().any()], X_test_ohe.isnull().any(axis = 1).sum())\n",
    "\n",
    "X_test_ohe_imputed = X_test_ohe.copy()\n",
    "\n",
    "corr_score_test = X_test_ohe_imputed.corrwith(y_test, axis=0, drop=True, method='pearson')\n",
    "corr_score_test = corr_score_test.rename_axis(\"variable\").reset_index()\n",
    "corr_score_test.rename(columns={ corr_score_test.columns[1]: \"Corr Score\" }, inplace = True)\n",
    "top20_test_df = corr_score_test.sort_values(by = \"Corr Score\", ascending = False, key = abs).iloc[:20]\n",
    "print(top20_test_df)\n",
    "\n",
    "X_test_t20 = X_test_ohe_imputed.loc[ :,column_list]\n",
    "print(X_test_t20.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa17f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (X_train_t20.shape,X_test_t20.shape)==((76050, 20), (25350, 20))\n",
    "assert len(X_train_t20.columns)== len(X_test_t20.columns) and len(X_train_t20.columns) == sum([1 for i, j in zip(X_train_t20.columns, X_test_t20.columns) if i == j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06869587",
   "metadata": {},
   "source": [
    "TASK 8:\n",
    "Apply min-max scaling on the training dataset (X_train_t20 obtained from Task 7). Save the result as X_train_scaled_mm.\n",
    "Then scale the test dataset (X_test_t20 obtained from Task 7) based on the metrics you obtain when you scale the training dataset. Save the result as X_test_scaled_mm.\n",
    "PLEASE DO NOT SCALE y_train and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d68198e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74074074 0.51020408 0.32653061 ... 0.         0.         0.        ]\n",
      " [0.85185185 0.51020408 0.26530612 ... 0.         0.         0.        ]\n",
      " [0.81481481 0.3877551  0.2244898  ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.77777778 0.30612245 0.20408163 ... 0.         0.         0.        ]\n",
      " [0.74074074 0.23469388 0.26530612 ... 0.         0.         0.        ]\n",
      " [0.77777778 0.24489796 0.20408163 ... 0.         0.         0.        ]]\n",
      "(76050, 20)\n",
      "[[0.77777778 0.33673469 0.32653061 ... 0.         0.         0.        ]\n",
      " [0.77777778 0.20408163 0.20408163 ... 0.         1.         0.        ]\n",
      " [0.74074074 0.34693878 0.24489796 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.85185185 0.28571429 0.06122449 ... 0.         0.         0.        ]\n",
      " [0.66666667 0.29591837 0.20408163 ... 0.         0.         0.        ]\n",
      " [0.74074074 0.30612245 0.24489796 ... 0.         0.         0.        ]]\n",
      "(25350, 20)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled_mm = np.array(X_train_t20)\n",
    "X_test_scaled_mm = np.array(X_test_t20)\n",
    "\n",
    "#Your task below:\n",
    "# YOUR CODE HERE\n",
    "\n",
    "X_train_scaled_mm = MinMaxScaler().fit_transform(X_train_t20)\n",
    "print(X_train_scaled_mm)\n",
    "print(X_train_scaled_mm.shape)\n",
    "\n",
    "X_test_scaled_mm = MinMaxScaler().fit_transform(X_test_t20)\n",
    "print(X_test_scaled_mm)\n",
    "print(X_test_scaled_mm.shape)\n",
    "\n",
    "# X_train_scaled_mm = (X_train_scaled_mm - X_train_t20.min().min())/(X_train_t20.max().max() - X_train_t20.min().min())\n",
    "# print(X_train_scaled_mm)\n",
    "# print(X_train_scaled_mm.shape)\n",
    "\n",
    "# X_test_scaled_mm = (X_test_scaled_mm - X_test_t20.min().min())/(X_test_t20.max().max() - X_test_t20.min().min())\n",
    "# print(X_test_scaled_mm)\n",
    "# print(X_test_scaled_mm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8d6b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(sum(np.array([min(X_train_scaled_mm[:,0]),max(X_train_scaled_mm[:,0]),min(X_test_scaled_mm[:,0]),max(X_test_scaled_mm[:,0])])-np.array([0.0,1.0,0.0,1.0])))<1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04178b",
   "metadata": {},
   "source": [
    "TASK 9:\n",
    "Apply standardization (i.e., normalization) scaling on the training dataset (X_train_t20 obtained from Task 7). Save the result as X_train_scaled_std.\n",
    "Then scale the test dataset (X_test_t20 obtained from Task 7) based on the metrics you obtain when you scale the training dataset. Save the result as X_test_scaled_std.\n",
    "PLEASE DO NOT SCALE y_train and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b0c4258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29311281  1.45317334  0.95158568 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [ 0.90339477  1.45317334  0.14891977 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [ 0.50455891  0.5703685  -0.38619084 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " ...\n",
      " [ 0.10572305 -0.01816806 -0.65374615 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [-0.29311281 -0.53313756  0.14891977 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]\n",
      " [ 0.10572305 -0.45957049 -0.65374615 ... -0.12308901 -0.1183216\n",
      "  -0.0582307 ]]\n",
      "[[ 0.10507939  0.19067582  0.96752115 ... -0.12069863 -0.11763412\n",
      "  -0.06035238]\n",
      " [ 0.10507939 -0.75951723 -0.65291564 ... -0.12069863  8.500935\n",
      "  -0.06035238]\n",
      " [-0.29518546  0.26376759 -0.11277004 ... -0.12069863 -0.11763412\n",
      "  -0.06035238]\n",
      " ...\n",
      " [ 0.90560909 -0.17478305 -2.54342522 ... -0.12069863 -0.11763412\n",
      "  -0.06035238]\n",
      " [-1.09571515 -0.10169127 -0.65291564 ... -0.12069863 -0.11763412\n",
      "  -0.06035238]\n",
      " [-0.29518546 -0.0285995  -0.11277004 ... -0.12069863 -0.11763412\n",
      "  -0.06035238]]\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled_std = np.array(X_train_t20)\n",
    "X_test_scaled_std = np.array(X_test_t20)\n",
    "\n",
    "#Your task below:\n",
    "# YOUR CODE HERE\n",
    "\n",
    "X_train_scaled_std = StandardScaler().fit_transform(X_train_t20)\n",
    "print(X_train_scaled_std)\n",
    "\n",
    "X_test_scaled_std = StandardScaler().fit_transform(X_test_t20)\n",
    "print(X_test_scaled_std)\n",
    "\n",
    "\n",
    "# X_train_scaled_std =(X_train_scaled_std-X_train_scaled_std.mean())/X_train_scaled_std.std()\n",
    "# print(X_train_scaled_std)\n",
    "\n",
    "# X_test_scaled_std =(X_test_scaled_std-X_test_scaled_std.mean())/X_test_scaled_std.std()\n",
    "# print(X_test_scaled_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b66167e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(20):\n",
    "    assert abs(X_train_scaled_std[:,i].mean()-0.0)<1e-4 and abs(X_train_scaled_std[:,i].std()-1.0)<1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70d582",
   "metadata": {},
   "source": [
    "TASK 10:\n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "\n",
    "linear_regression_closed_form_training : It fits a linear regression model using the closed-form solution to obtain the coefficients, beta's, as a numpy array of m+1 values (Please recall class lecture), where m is the number of variables kept in X_train (the first argument to the function). Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the cpu_time.\n",
    "\n",
    "linear_regression_closed_form_predict: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having m input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function.\n",
    "\n",
    "RMSLE: It takes two lists: y_test, y_pred, where the first list represents ground truth (i.e., actual) target values for the given samples, and the second list represents a corresponding predicted values for exactly same number of samples in y_test. Compute and return the Root Mean Squared Logarithmic Error (RMSLE) of the prediction.\n",
    "\n",
    "PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION or RMSLE calculation.\n",
    "\n",
    "Now, call linear_regression_closed_form_training() function providing X_train_scaled_std, y_train obtained from Task 9, and save the returned results as betas_closed_form,cpu_time_closed_form.\n",
    "\n",
    "Print betas_closed_form, cpu_time_closed_form\n",
    "\n",
    "Call linear_regression_closed_form_predict() function providing betas_closed_form,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "\n",
    "Call RMSLE() function providing y_test and y_pred. Save returned result as rmsle_closed_form.\n",
    "\n",
    "Print rmsle_closed_form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deae45ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_closed_form_training(X_train, y_train):\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "\n",
    "    #Your work here\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    start_time = perf_counter()\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train['bias'] = 1 \n",
    "    beta = np.dot((np.linalg.inv(np.dot(X_train.T, X_train))), np.dot(X_train.T,y_train))\n",
    "    betas.append(beta)\n",
    "    \n",
    "    end_time = perf_counter()\n",
    "    cpu_time = end_time - start_time\n",
    "    \n",
    "    \n",
    "    return betas,cpu_time\n",
    "\n",
    "def linear_regression_closed_form_predict(betas, X_test):\n",
    "    y_pred = []\n",
    "    #your work below\n",
    "    # YOUR CODE HERE\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test['bias'] = 1\n",
    "    \n",
    "    y_pred = np.dot(X_test, betas)\n",
    "    return y_pred\n",
    "\n",
    "def RMSLE(y_test, y_pred, verbose=False):\n",
    "    rmsle_val = 0\n",
    "    #Your work below\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    y_test = np.array(y_test)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = abs(y_pred)\n",
    "    rmsle_val = np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_test))))\n",
    "    return rmsle_val\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5af6b0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression CPU Time: 0.04536215799999965\n",
      "Beta Values are:\n",
      " [ 0.71519322  0.16451626  0.03403515 -0.08264272 -0.07634788 -0.12961155\n",
      " -0.09253333 -0.05847441 -0.04475425 -0.03880411 -0.02739149  0.10283456\n",
      "  0.0723325  -0.03912292 -0.01662473  0.00921883 -0.01519333 -0.04398384\n",
      " -0.02638443 -0.01920356  7.25699786]\n"
     ]
    }
   ],
   "source": [
    "betas_closed_form = []\n",
    "cpu_time_closed_form = 0\n",
    "#Your task here, call appropriate function to get these two\n",
    "# YOUR CODE HERE\n",
    "betas_closed_form, cpu_time_closed_form  = linear_regression_closed_form_training(X_train_scaled_std, y_train)\n",
    "print(\"Linear regression CPU Time:\",cpu_time_closed_form)\n",
    "betas_closed_form = [i for blist in betas_closed_form for i in blist]\n",
    "betas_closed_form = np.array(betas_closed_form)\n",
    "print(\"Beta Values are:\\n\",betas_closed_form)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e16e136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.38676756, 6.76686327, 6.98815652, ..., 7.93817651, 6.61901319,\n",
       "       7.43494415])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = []\n",
    "#Your task here. call appropriate function to get get y prediction for the supplied test dataset scaled\n",
    "# YOUR CODE HERE\n",
    "y_pred = linear_regression_closed_form_predict(betas_closed_form, X_test_scaled_std)\n",
    "#y_pred = [i for blist in y_pred for i in blist]\n",
    "#y_pred = pd.DataFrame(y_pred)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "876a87a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13893479990000063\n"
     ]
    }
   ],
   "source": [
    "rmsle_closed_form = 0\n",
    "#Your task below. Call the function to compute RMSLE score of the y predictions\n",
    "# YOUR CODE HERE\n",
    "rmsle_closed_form = RMSLE(y_test, y_pred)\n",
    "print(rmsle_closed_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c829d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(RMSLE([1,2,3,4,5],[1,1,1,1,1])-0.733674673672524)<1e-4\n",
    "assert abs(RMSLE([100, 200, 300, 400, 500], [90, 190, 290, 390, 490])-0.05596497907273607)<1e-4\n",
    "assert (betas_closed_form.shape[0],y_pred.shape[0])==(21,25350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ed9b4",
   "metadata": {},
   "source": [
    "TASK 11:\n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "\n",
    "linear_regression_gd_batch_training : It fits a linear regression model using the batch gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where m is the number of variables kept in X_train (the first argument to the function). Make sure you compute average of gradients in the batch. Please use the alpha (i.e, the learning rate) and nEpoch (number of epochs) parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the cpu_time.\n",
    "\n",
    "linear_regression_gd_batch_predict: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having m input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function.\n",
    "\n",
    "PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "\n",
    "Now, call linear_regression_gd_batch_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.01,nEpoch=1000, and save the returned results as betas_batch,cpu_time_batch.\n",
    "\n",
    "Print betas_batch, cpu_time_batch\n",
    "\n",
    "Call linear_regression_gd_batch_predict() function providing betas_batch,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "\n",
    "Call RMSLE() function providing y_test and y_pred. Save returned result as rmsle_batch.\n",
    "\n",
    "Print rmsle_batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9210954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gd_batch_training(X_train,y_train, alpha, nEpoch):\n",
    "    random.seed(554433)\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "    epsilon = 1e-6\n",
    "    #Your task below\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    startTime = perf_counter()\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train['bias'] = 1\n",
    "    \n",
    "    feature_count = X_train.shape[1]\n",
    "    betas = np.ones(shape = feature_count)\n",
    "    \n",
    "    m = X_train.shape[0]\n",
    "    \n",
    "    for i in range(nEpoch):\n",
    "        y_predict = np.dot(betas, X_train.T)\n",
    "        grad = -(2/m) * (np.dot(X_train.T,(y_train - y_predict)))\n",
    "        betas = betas - alpha * grad\n",
    "    \n",
    "    endTime = perf_counter()\n",
    "    cpu_time = endTime - startTime\n",
    "    \n",
    "    return betas, cpu_time\n",
    "\n",
    "def linear_regression_gd_batch_predict(betas,X_test):\n",
    "    y_pred = []\n",
    "    \n",
    "    #Your task below\n",
    "    # YOUR CODE HERE\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test['bias'] = 1\n",
    "    y_pred = (np.dot(X_test, betas))\n",
    "    return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5c87ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Batch time = 7.305574580000002\n",
      "Beta Values are:\n",
      "            0\n",
      "0   0.715199\n",
      "1   0.164523\n",
      "2   0.034034\n",
      "3  -0.082642\n",
      "4  -0.076325\n",
      "5  -0.129610\n",
      "6  -0.092523\n",
      "7  -0.058669\n",
      "8  -0.044552\n",
      "9  -0.038804\n",
      "10 -0.027392\n",
      "11  0.103042\n",
      "12  0.072332\n",
      "13 -0.039122\n",
      "14 -0.016642\n",
      "15  0.009027\n",
      "16 -0.015195\n",
      "17 -0.043983\n",
      "18 -0.026386\n",
      "19 -0.019204\n",
      "20  7.256998\n"
     ]
    }
   ],
   "source": [
    "betas_batch = []\n",
    "cpu_time_batch = 0\n",
    "\n",
    "#your task below\n",
    "# YOUR CODE HERE\n",
    "alpha=0.01\n",
    "nEpoch=1000\n",
    "\n",
    "betas_batch,cpu_time_batch = linear_regression_gd_batch_training(X_train_scaled_std, y_train, alpha, nEpoch)\n",
    "\n",
    "print(\"Linear Regression Batch time =\", cpu_time_batch)\n",
    "betas_batch = pd.DataFrame(betas_batch)\n",
    "print(\"Beta Values are:\\n\",betas_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b6dee19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.386779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.766822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.988124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.201842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.285637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25345</th>\n",
       "      <td>7.195200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25346</th>\n",
       "      <td>7.421599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25347</th>\n",
       "      <td>7.938386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25348</th>\n",
       "      <td>6.620237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25349</th>\n",
       "      <td>7.435030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25350 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0      7.386779\n",
       "1      6.766822\n",
       "2      6.988124\n",
       "3      7.201842\n",
       "4      7.285637\n",
       "...         ...\n",
       "25345  7.195200\n",
       "25346  7.421599\n",
       "25347  7.938386\n",
       "25348  6.620237\n",
       "25349  7.435030\n",
       "\n",
       "[25350 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "#your task below\n",
    "# YOUR CODE HERE\n",
    "y_pred = linear_regression_gd_batch_predict(betas_batch, X_test_scaled_std)\n",
    "y_pred = [i for blist in y_pred for i in blist]\n",
    "y_pred = pd.DataFrame(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d4f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2248902523028116\n"
     ]
    }
   ],
   "source": [
    "rmsle_batch = 0\n",
    "# YOUR CODE HERE\n",
    "rmsle_batch = RMSLE(y_test, y_pred)\n",
    "print(rmsle_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10194f",
   "metadata": {},
   "source": [
    "## TASK 12:\n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "* **linear_regression_gd_stochastic_training** : It fits a linear regression model using the stochastic gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where *m* is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate), nEpoch (number of epochs), nIteration (number of iterations) parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the  cpu_time.\n",
    "* **linear_regression_gd_stochastic_predict**: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having *m* input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function. \n",
    "\n",
    "* PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "* Now, call linear_regression_gd_stochastic_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.001,nEpoch=100, nIteration=10000 , and save the returned results as betas_stochastic,cpu_time_stochastic.\n",
    "* Print betas_stochastic, cpu_time_stochastic\n",
    "* Call linear_regression_gd_stochastic_predict() function providing betas_stochastic,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "* Call RMSLE() function providing y_test and y_pred. Save returned result as rmsle_stochastic.\n",
    "* Print rmsle_stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccd38f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gd_stochastic_training(X_train,y_train,alpha,nEpoch,nIteration):\n",
    "    random.seed(554433)\n",
    "    betas = []\n",
    "    cpu_time = 0\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    startTime = perf_counter()\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train['bias'] = 1\n",
    "    m = X_train.shape[0]\n",
    "    \n",
    "    feature_count = X_train.shape[1]\n",
    "    betas = np.ones(shape = feature_count)\n",
    "    \n",
    "    for i in range(nEpoch):\n",
    "        temp = random.randint(0,m-1)\n",
    "        temp_x = X_train.iloc[temp]\n",
    "        temp_y = y_train.iloc[temp]\n",
    "        y_predict = np.dot(betas, temp_x.T) \n",
    "        grad = -(2/m) * (np.dot(temp_x.T,(temp_y - y_predict)))\n",
    "        betas = betas - alpha * grad\n",
    "        \n",
    "    endTime = perf_counter()\n",
    "    cpu_time = endTime - startTime\n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = perf_counter()\n",
    "    #n, m = X_train.shape\n",
    "    for i in range(nIteration):\n",
    "        start_time = perf_counter()\n",
    "        #n, m = X_train.shape\n",
    "        #betas with shape (m+1,1)\n",
    "        m,n = X_train.shape\n",
    "        #beta = np.random.rand(n) #X_train_scaled_std, y_train\n",
    "        betas_stochastic = inv(X_train_scaled_std.T.dot(X_train_scaled_std)).dot(X_train_scaled_std.T).dot(y_train)\n",
    "    for _ in range(nIteration):\n",
    "        x = \"-\".join(str(n) for n in range(nEpoch))\n",
    "        end = perf_counter()\n",
    "    #rint('Perf Counter= ', end-start_time)\n",
    "    cpu_time = end-start_time\n",
    "    \n",
    "    return betas, cpu_time\n",
    "\n",
    "def linear_regression_gd_stochastic_predict(betas, X_test):\n",
    "    y_pred = []\n",
    "    \n",
    "    #your code below\n",
    "    # YOUR CODE HERE\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test['bias'] = 1\n",
    "    y_pred = np.dot(X_test, betas)\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61fa1366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 0.028914396000004672\n",
      "Betas:\n",
      "           0\n",
      "0   0.999999\n",
      "1   0.999998\n",
      "2   0.999997\n",
      "3   0.999997\n",
      "4   1.000003\n",
      "5   0.999997\n",
      "6   0.999998\n",
      "7   0.999997\n",
      "8   0.999997\n",
      "9   0.999994\n",
      "10  0.999999\n",
      "11  0.999994\n",
      "12  1.000000\n",
      "13  0.999999\n",
      "14  0.999993\n",
      "15  0.999994\n",
      "16  0.999994\n",
      "17  0.999998\n",
      "18  0.999997\n",
      "19  0.999999\n",
      "20  1.000016\n"
     ]
    }
   ],
   "source": [
    "betas_stochastic = []\n",
    "cpu_time_stochastic = 0\n",
    "#your code below\n",
    "# YOUR CODE HERE\n",
    "alpha=0.001\n",
    "nEpoch=100\n",
    "nIteration=1000\n",
    "betas_stochastic, cpu_time_stochastic = linear_regression_gd_stochastic_training(X_train_scaled_std,y_train,alpha,nEpoch,nIteration)\n",
    "print(\"CPU Time:\", cpu_time_stochastic)\n",
    "betas_stochastic = pd.DataFrame(betas_stochastic)\n",
    "print(\"Betas:\")\n",
    "print(betas_stochastic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80d2255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Prediction:\n",
      "[[ 3.38063447]\n",
      " [ 6.76612078]\n",
      " [-1.55878885]\n",
      " ...\n",
      " [-3.13178069]\n",
      " [ 6.91674302]\n",
      " [ 2.0585277 ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "#your code below\n",
    "# YOUR CODE HERE\n",
    "y_pred = linear_regression_gd_stochastic_predict(betas_stochastic, X_test_scaled_std)\n",
    "print(\"Y Prediction:\")\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05ad43ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2248902523028116\n"
     ]
    }
   ],
   "source": [
    "rmsle_stochastic = 0\n",
    "#your code below\n",
    "# YOUR CODE HERE\n",
    "rmsle_stochastic = RMSLE(y_test, y_pred)\n",
    "print(rmsle_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dba4de",
   "metadata": {},
   "source": [
    "Task 13:\n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively: complete the three function definitions and demonstrate the functionalities of each by calling them with appropriate arguments as instructed below:\n",
    "\n",
    "linear_regression_gd_minibatch_training : It fits a linear regression model using the minibatch gradient descent algorithm to obtain the coefficients, beta's, as a numpy array of m+1 values, where m is the number of variables kept in X_train (the first argument to the function). Please use the alpha (i.e, the learning rate), nEpoch (number of epochs), nIteration (number of iterations), and batch_size parameters in your implementation of the gradient descent algorithm. Please measure the cpu_time needed during the training step. cpu_time is not equal to the wall_time. So, use time.perf_counter() for an accurate measurement. Documentation on this function can be found here: https://docs.python.org/3/library/time.html . Finally, the function returns betas (i.e., the m+1 beta values) and the cpu_time.\n",
    "\n",
    "linear_regression_gd_minibatch_predict: It takes a list of m+1 beta values (i.e., betas returned from the corresponding training function, and X_test (containing test samples each having m input variables). Now, using the provided beta values, predict each of the test samples provided, and let's name your prediction \"y_pred\". Return y_pred from the function.\n",
    "\n",
    "PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION.\n",
    "\n",
    "Now, call linear_regression_gd_minibatch_training() function providing X_train_scaled_std, y_train obtained from Task 9, and alpha=0.01,nEpoch=50, nIteration=1000,batch_size=32, and save the returned results as betas_minibatch,cpu_time_minibatch.\n",
    "\n",
    "Print betas_minibatch, cpu_time_minibatch\n",
    "\n",
    "Call linear_regression_gd_minibatch_predict() function providing betas_minibatch,X_test_scaled_std obtained in Task 9. Save the returned result as y_pred.\n",
    "\n",
    "Call RMSLE() function providing y_test and y_pred. Save returned result as rmsle_minibatch.\n",
    "\n",
    "Print rmsle_minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebcd70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gd_minibatch_training(X_train,y_train,alpha,nEpoch, nIteration, batch_size):\n",
    "    random.seed(554433)\n",
    "    cpu_time = 0\n",
    "    epsilon = 1e-6\n",
    "    betas = []\n",
    "    ## your code below\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    startTime = perf_counter()\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    m = X_train.shape[0]\n",
    "    \n",
    "    X_train['bias'] = 1\n",
    "    feature_count = X_train.shape[1]\n",
    "    \n",
    "    #betas must be initialized with any value. This function will obtain the optimum beta values by iterating through nEpoch\n",
    "    betas = np.ones(shape = feature_count)\n",
    "    \n",
    "    if(batch_size >= m):\n",
    "        batch_size = m\n",
    "    \n",
    "    number_of_batches = int(m/batch_size)\n",
    "    \n",
    "    for i in range (nEpoch):\n",
    "        temp = np.random.permutation(m)\n",
    "        \n",
    "        temp_x = X_train.iloc[temp]\n",
    "        temp_y = y_train.iloc[temp]\n",
    "        \n",
    "        for j in range (0, m, batch_size):\n",
    "            batch_x = temp_x[j:j+batch_size]\n",
    "            batch_y = temp_y[j:j+batch_size]\n",
    "            y_predict = np.dot(betas, batch_x.T)\n",
    "            grad = -(2/m) * (np.dot(batch_x.T,(batch_y - y_predict)))\n",
    "            betas = betas - alpha * grad\n",
    "        \n",
    "    endTime = perf_counter()\n",
    "    cpu_time = endTime - startTime\n",
    "    \n",
    "    return betas,cpu_time\n",
    "\n",
    "def linear_regression_gd_minibatch_predict(betas,X_test):\n",
    "    #your solution below\n",
    "    y_pred = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test['bias'] = 1\n",
    "    y_pred = np.dot(X_test, betas)\n",
    "    \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dc5d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 99.46170933399998\n",
      "Betas:\n",
      "           0\n",
      "0   0.949977\n",
      "1   0.424020\n",
      "2   0.303324\n",
      "3   0.292158\n",
      "4   0.651022\n",
      "5   0.293780\n",
      "6   0.358615\n",
      "7   0.110519\n",
      "8   0.129453\n",
      "9   0.290265\n",
      "10  0.318634\n",
      "11  0.266705\n",
      "12  0.413039\n",
      "13  0.314207\n",
      "14  0.160562\n",
      "15  0.214740\n",
      "16  0.155814\n",
      "17  0.312966\n",
      "18  0.317729\n",
      "19  0.345461\n",
      "20  4.955185\n"
     ]
    }
   ],
   "source": [
    "betas_minibatch = []\n",
    "cpu_time_minibatch = 0\n",
    "#your task below\n",
    "\n",
    "# YOUR CODE HERE\n",
    "alpha=0.01\n",
    "nEpoch=50\n",
    "nIteration=1000\n",
    "batch_size=32\n",
    "\n",
    "betas_minibatch,cpu_time_minibatch = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "print(\"CPU Time:\",cpu_time_minibatch)\n",
    "print(\"Betas:\")\n",
    "betas_minibatch = pd.DataFrame(betas_minibatch)\n",
    "print(betas_minibatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2505ce12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Prediction:\n",
      "[[5.29087717]\n",
      " [6.46789182]\n",
      " [4.60066486]\n",
      " ...\n",
      " [4.38665844]\n",
      " [4.407233  ]\n",
      " [4.48286003]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "#your ans below\n",
    "# YOUR CODE HERE\n",
    "y_pred = linear_regression_gd_minibatch_predict(betas_minibatch,X_test_scaled_std)\n",
    "print(\"Y Prediction:\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2300c449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48803349517674804\n"
     ]
    }
   ],
   "source": [
    "rmsle_minibatch = 0\n",
    "#your ans below\n",
    "# YOUR CODE HERE\n",
    "rmsle_minibatch = RMSLE(y_test, y_pred)\n",
    "print(rmsle_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a9a0f",
   "metadata": {},
   "source": [
    "Task 14:\n",
    "Given the 4 sets of results from the 4 experiments (from Tasks 10, 11, 12, 13) with closed form solution, batch gradient descent, stochastic gradient descent and mini-batch gradient descent, assign a string from the set {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the best predictive performance in terms of RMSE to a variable named best_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79a509c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Least RMSLE is: 0.13893479990000063\n",
      "\n",
      "Best name is : closed-form\n"
     ]
    }
   ],
   "source": [
    "names = [\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"]\n",
    "rmsle_vals = [rmsle_closed_form, rmsle_batch, rmsle_stochastic, rmsle_minibatch]\n",
    "\n",
    "best_name = 'xxxx'\n",
    "# YOUR CODE HERE\n",
    "\n",
    "minimum = rmsle_vals[0]\n",
    "best_name = names[0]\n",
    " \n",
    "for i in range(1, len(rmsle_vals)):\n",
    "   \n",
    "    if rmsle_vals[i] < minimum:\n",
    "        minimum = rmsle_vals[i]\n",
    "        best_name = names[i]\n",
    "print(\"\\nLeast RMSLE is:\", minimum)\n",
    "print(\"\\nBest name is :\", best_name) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2b9a33",
   "metadata": {},
   "source": [
    "Task 15:\n",
    "Given the 4 sets of results from the 4 experiments (from Tasks 10, 11, 12, 13) with closed form solution, batch gradient descent, stochastic gradient descent and mini-batch gradient descent, assign a string from the set {\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"} that demonstrated the least training cpu time to a variable called best_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "071216ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Least Training CPU time is: 0.028914396000004672\n",
      "\n",
      "Best name is : stochastic-GD\n"
     ]
    }
   ],
   "source": [
    "names = [\"closed-form\", \"batch-GD\", \"stochastic-GD\", \"minibatch-GD\"]\n",
    "cpu_times = [cpu_time_closed_form, cpu_time_batch, cpu_time_stochastic, cpu_time_minibatch]\n",
    "best_name = 'xxxx'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "minimum = cpu_times[0]\n",
    "best_name = names[0]\n",
    " \n",
    "for i in range(1, len(cpu_times)):\n",
    "   \n",
    "    if cpu_times[i] < minimum:\n",
    "        minimum = cpu_times[i]\n",
    "        best_name = names[i]\n",
    "print(\"\\nLeast Training CPU time is:\", minimum)\n",
    "print(\"\\nBest name is :\", best_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc52fe",
   "metadata": {},
   "source": [
    "Task 16:\n",
    "Given the (X_train_scaled_std, y_train) pairs denoting input matrix and output vector respectively,\n",
    "\n",
    "call your implementation of Task 13: minibatch gradient descent based linear regression for each of these learning rates: {0.001, 0.01, 0.05}, batch sizes: {32, 64, 128, 256}\n",
    "\n",
    "Please use the nIteration (number of iterations)=100, nEpoch (number of epoch)=50.\n",
    "For each of the linear regression model, using the computed beta values, predict the test samples provided in the \"X_test_scaled_std\" argument, and let's name your prediction \"y_pred\".\n",
    "\n",
    "Compute Root Mean Squared Logarithmic Error (RMSLE) of your prediction using the RMSLE() function you defined in Task 10.\n",
    "\n",
    "Finally, assign the learning rate that shows the best test performance to a variable called best_alpha, and also assign as a pandas dataframe named summary with 3 columns: {learning_rate, batch_size, test_RMSLE} containing RMSLE's of the 12 linear regression models, i.e., Cartesian product between the alphas and batch_sizes. Also, assign the best performing batch_size to a variable best_batch_size.\n",
    "\n",
    "PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8578f246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n",
      "/var/folders/yy/pk8ymktn2997l12961d0jwnh0000gn/T/ipykernel_88703/2033779854.py:32: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  batch_y = temp_y[j:j+batch_size]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    learning_rate  batch_size  test_RMSLE\n",
      "0           0.001          32    1.081976\n",
      "1           0.010          64    0.449556\n",
      "2           0.050         128    0.138313\n",
      "3           0.001         256    1.081976\n",
      "4           0.010          32    0.449558\n",
      "5           0.050          64    0.138316\n",
      "6           0.001         128    1.081976\n",
      "7           0.010         256    0.449541\n",
      "8           0.050          32    0.138316\n",
      "9           0.001          64    1.081976\n",
      "10          0.010         128    0.449550\n",
      "11          0.050         256    0.138315\n",
      "Best performing learning rate :  0.05\n",
      "Best performing batch size :  128\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.001, 0.01, 0.05]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "summary = pd.DataFrame(columns=['learning_rate', 'batch_size', 'test_RMSLE'])\n",
    "nIteration=1000\n",
    "nEpoch=50\n",
    "\n",
    "best_alpha = -1\n",
    "best_beta = []\n",
    "best_batch_size = -1\n",
    "\n",
    "#your code below\n",
    "# YOUR CODE HERE\n",
    "\n",
    "#1\n",
    "alpha = 0.001\n",
    "batch_size = 32\n",
    "betas_minibatch_0001_32,cpu_time_minibatch_0001_32 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_0001_32 = linear_regression_gd_minibatch_predict(betas_minibatch_0001_32,X_test_scaled_std)\n",
    "rmsle_minibatch_0001_32 = RMSLE(y_test, y_pred_0001_32)\n",
    "\n",
    "\n",
    "#2\n",
    "alpha = 0.01\n",
    "batch_size = 64\n",
    "betas_minibatch_001_64,cpu_time_minibatch_001_64 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_001_64 = linear_regression_gd_minibatch_predict(betas_minibatch_001_64,X_test_scaled_std)\n",
    "rmsle_minibatch_001_64 = RMSLE(y_test, y_pred_001_64)\n",
    "\n",
    "\n",
    "#3\n",
    "alpha = 0.05\n",
    "batch_size = 128\n",
    "betas_minibatch_005_128,cpu_time_minibatch_005_128 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_005_128 = linear_regression_gd_minibatch_predict(betas_minibatch_005_128,X_test_scaled_std)\n",
    "rmsle_minibatch_005_128 = RMSLE(y_test, y_pred_005_128)\n",
    "\n",
    "#4\n",
    "alpha = 0.001\n",
    "batch_size = 256\n",
    "betas_minibatch_0001_256,cpu_time_minibatch_0001_256 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_0001_256 = linear_regression_gd_minibatch_predict(betas_minibatch_0001_256,X_test_scaled_std)\n",
    "rmsle_minibatch_0001_256 = RMSLE(y_test, y_pred_0001_256)\n",
    "\n",
    "\n",
    "#5\n",
    "alpha = 0.01\n",
    "batch_size = 32\n",
    "betas_minibatch_001_32,cpu_time_minibatch_001_32 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_001_32 = linear_regression_gd_minibatch_predict(betas_minibatch_001_32,X_test_scaled_std)\n",
    "rmsle_minibatch_001_32 = RMSLE(y_test, y_pred_001_32)\n",
    "\n",
    "\n",
    "#6\n",
    "alpha = 0.05\n",
    "batch_size = 64\n",
    "betas_minibatch_005_64,cpu_time_minibatch_005_64 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_005_64 = linear_regression_gd_minibatch_predict(betas_minibatch_005_64,X_test_scaled_std)\n",
    "rmsle_minibatch_005_64 = RMSLE(y_test, y_pred_005_64)\n",
    "\n",
    "\n",
    "#7\n",
    "alpha = 0.001\n",
    "batch_size = 128\n",
    "betas_minibatch_0001_128,cpu_time_minibatch_0001_128 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_0001_128 = linear_regression_gd_minibatch_predict(betas_minibatch_0001_128,X_test_scaled_std)\n",
    "rmsle_minibatch_0001_128 = RMSLE(y_test, y_pred_0001_128)\n",
    "\n",
    "\n",
    "#8\n",
    "alpha = 0.01\n",
    "batch_size = 256\n",
    "betas_minibatch_001_256,cpu_time_minibatch_001_256 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_001_256 = linear_regression_gd_minibatch_predict(betas_minibatch_001_256,X_test_scaled_std)\n",
    "rmsle_minibatch_001_256 = RMSLE(y_test, y_pred_001_256)\n",
    " \n",
    "\n",
    "#9\n",
    "alpha = 0.05\n",
    "batch_size = 32\n",
    "betas_minibatch_005_32,cpu_time_minibatch_005_32 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_005_32 = linear_regression_gd_minibatch_predict(betas_minibatch_005_32,X_test_scaled_std)\n",
    "rmsle_minibatch_005_32 = RMSLE(y_test, y_pred_005_32)\n",
    "\n",
    "\n",
    "#10\n",
    "alpha = 0.001\n",
    "batch_size = 64\n",
    "betas_minibatch_0001_64,cpu_time_minibatch_0001_64 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_0001_64 = linear_regression_gd_minibatch_predict(betas_minibatch_0001_64,X_test_scaled_std)\n",
    "rmsle_minibatch_0001_64 = RMSLE(y_test, y_pred_0001_64)\n",
    "\n",
    "#11\n",
    "alpha = 0.01\n",
    "batch_size = 128\n",
    "betas_minibatch_001_128,cpu_time_minibatch_001_128 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_001_128 = linear_regression_gd_minibatch_predict(betas_minibatch_001_128,X_test_scaled_std)\n",
    "rmsle_minibatch_001_128 = RMSLE(y_test, y_pred_001_128)\n",
    "\n",
    "#12\n",
    "alpha = 0.05\n",
    "batch_size = 256\n",
    "betas_minibatch_005_256,cpu_time_minibatch_001_256 = linear_regression_gd_minibatch_training(X_train_scaled_std, y_train, alpha, nEpoch, nIteration, batch_size)\n",
    "y_pred_005_256 = linear_regression_gd_minibatch_predict(betas_minibatch_005_256,X_test_scaled_std)\n",
    "rmsle_minibatch_005_256 = RMSLE(y_test, y_pred_005_256)\n",
    "\n",
    "\n",
    "summary = pd.DataFrame ({'learning_rate' : [0.001, 0.01, 0.05, 0.001, 0.01, 0.05, 0.001, 0.01, 0.05, 0.001, 0.01, 0.05], \n",
    "                         'batch_size' : [32, 64, 128, 256, 32, 64, 128, 256, 32, 64, 128, 256], \n",
    "                         'test_RMSLE' :[rmsle_minibatch_0001_32, \n",
    "                                        rmsle_minibatch_001_64, \n",
    "                                        rmsle_minibatch_005_128,\n",
    "                                        rmsle_minibatch_0001_256,\n",
    "                                        rmsle_minibatch_001_32,\n",
    "                                        rmsle_minibatch_005_64,\n",
    "                                        rmsle_minibatch_0001_128,\n",
    "                                        rmsle_minibatch_001_256,\n",
    "                                        rmsle_minibatch_005_32,\n",
    "                                        rmsle_minibatch_0001_64,\n",
    "                                        rmsle_minibatch_001_128,\n",
    "                                        rmsle_minibatch_005_256]})\n",
    "\n",
    "\n",
    "print(summary)\n",
    "\n",
    "best_alpha = summary.iloc[summary['test_RMSLE'].idxmin(),0]\n",
    "best_batch_size = summary.iloc[summary['test_RMSLE'].idxmin(),1]\n",
    "\n",
    "print(\"Best performing learning rate : \" , best_alpha)\n",
    "print(\"Best performing batch size : \" , best_batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19ac4c",
   "metadata": {},
   "source": [
    "Task 17:\n",
    "Utilizing the best trained linear regression model (so far), predict the target for each of the samples in the judge_dataset.\n",
    "I believe you will not forget to do the following before call in the prediction algorithm:\n",
    "Save the ID values of the judge dataset into ID_judge and drop it from the judge dataframe.\n",
    "Perform onehot encoding using the same encoder you used to encode X_test (Task 4).\n",
    "keep only the same top 20 variables as you did in Task 7.\n",
    "scale the input variables based on the same metrics you used to scale the training dataset (Task 9).\n",
    "Now, call the prediction function of that model to obtain y_pred.\n",
    "Prepare a pandas dataframe called result having columns: {ID, BWEIGHT}, where ID will the ID of the judge sample, and BWEIGHT is the corresponding y_pred value from your model prediction.\n",
    "Save the dataframe as my_submission.csv.\n",
    "PLEASE DO NOT USE ANY LIBRARY FUNCTION THAT DOES THE LINEAR REGRESSION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d942b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      WEEKS  GAINED  VISITS  HYPERPR  MARITAL  SEX  CIGNUM  RACEDAD  RACEMOM  \\\n",
      "0        36      70      11        0        1    1       0        1        1   \n",
      "1        41      36      15        0        2    2       0        3        3   \n",
      "2        39      18      10        0        1    2      20        1        1   \n",
      "3        39      25      10        0        1    2       0        1        1   \n",
      "4        38      38      15        1        1    2       0        1        1   \n",
      "...     ...     ...     ...      ...      ...  ...     ...      ...      ...   \n",
      "1995     35      17       9        0        1    2       0        1        1   \n",
      "1996     39      20      10        0        1    1       0        1        1   \n",
      "1997     40      25      17        0        1    1       0        1        1   \n",
      "1998     38      29      11        0        1    1       0        1        1   \n",
      "1999     34      20      13        0        2    1       5        1        1   \n",
      "\n",
      "      PRETERM  CERVIX  MAGE  PINFANT  ECLAMP  MEDUC  FAGE  FEDUC  HYDRAM  \\\n",
      "0           0       0    26        0       0     16    30     14       0   \n",
      "1           0       0    18        0       0     12    21      9       0   \n",
      "2           0       0    25        0       0     12    22     12       0   \n",
      "3           0       0    22        0       0     11    24     12       0   \n",
      "4           0       0    26        0       0     12    24     12       0   \n",
      "...       ...     ...   ...      ...     ...    ...   ...    ...     ...   \n",
      "1995        0       0    21        0       0     12    21     13       0   \n",
      "1996        0       0    26        0       0     16    27     16       0   \n",
      "1997        0       0    27        0       0     12    29     12       0   \n",
      "1998        0       0    34        0       0     16    37     12       0   \n",
      "1999        0       0    27        0       0     13    30     12       0   \n",
      "\n",
      "      HYPERCH  UTERINE  \n",
      "0           0        0  \n",
      "1           0        0  \n",
      "2           0        0  \n",
      "3           0        0  \n",
      "4           0        0  \n",
      "...       ...      ...  \n",
      "1995        0        0  \n",
      "1996        0        0  \n",
      "1997        0        0  \n",
      "1998        0        0  \n",
      "1999        0        0  \n",
      "\n",
      "[2000 rows x 20 columns]\n",
      "[[-1.11484766  2.90008064 -0.38853968 ... -0.11476596 -0.12952535\n",
      "  -0.05006262]\n",
      " [ 0.86957324  0.41647821  0.69790473 ... -0.11476596 -0.12952535\n",
      "  -0.05006262]\n",
      " [ 0.07580488 -0.89837013 -0.66015079 ... -0.11476596 -0.12952535\n",
      "  -0.05006262]\n",
      " ...\n",
      " [ 0.47268906 -0.38704022  1.24112694 ... -0.11476596 -0.12952535\n",
      "  -0.05006262]\n",
      " [-0.3210793  -0.0948517  -0.38853968 ... -0.11476596 -0.12952535\n",
      "  -0.05006262]\n",
      " [-1.90861602 -0.75227587  0.15468252 ... -0.11476596 -0.12952535\n",
      "  -0.05006262]]\n"
     ]
    }
   ],
   "source": [
    "#your task below\n",
    "#i) store the id in a separate variable and drop it from judge dataset\n",
    "#ii) use onehot encoder you obtained to encode the judge set\n",
    "#iii) keep only the top-20 variables in the judge set\n",
    "#iv) scale using the same scalar you obtained before, and save it to judge_scaled_std variable.\n",
    "judge_scaled_std = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Save the ID values of the judge dataset into ID_judge and drop it from the judge dataframe.\n",
    "X_judge = judge_dataset\n",
    "ID_judge = X_judge[\"ID\"]\n",
    "X_judge = X_judge.drop(\"ID\", axis=1)\n",
    "\n",
    "\n",
    "# Perform onehot encoding using the same encoder you used to encode X_test (Task 4).\n",
    "X_judge_ohe, enc_list, categorical_features = lets_do_one_hot_encoding(X_judge ,categorical_features, transform_only=True, encoders=enc_list)\n",
    "\n",
    "# keep only the same top 20 variables as you did in Task 7.\n",
    "X_judge_t20 = pd.DataFrame()\n",
    "for i in range(len(top20_df)):\n",
    "    try:\n",
    "        cols = X_judge_ohe.iloc[:,X_judge_ohe.columns.get_loc(top20_df.iloc[i,0])]\n",
    "        X_judge_t20[top20_df.iloc[i,0]] = cols\n",
    "    except KeyError:\n",
    "        pass\n",
    "print(X_judge_t20)\n",
    "\n",
    "# scale the input variables based on the same metrics you used to scale the training dataset (Task 9).\n",
    "judge_scaled_std = StandardScaler().fit_transform(X_judge_t20)\n",
    "print(judge_scaled_std)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed0b09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your task is to predict using your best regression betas, and save it to `y_pred` variable\n",
    "y_pred = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "y_pred_batch = linear_regression_gd_batch_predict(betas_batch,judge_scaled_std)\n",
    "#y_pred_linear = linear_regression_closed_form_predict(betas_closed_form, judge_scaled_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbe76956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID   BWEIGHT\n",
      "0        1  7.143660\n",
      "1        2  7.516157\n",
      "2        3  6.525241\n",
      "3        4  7.148904\n",
      "4        5  6.766487\n",
      "...    ...       ...\n",
      "1995  1996  5.875826\n",
      "1996  1997  7.370508\n",
      "1997  1998  7.843530\n",
      "1998  1999  7.376166\n",
      "1999  2000  5.726769\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#your task below is to prepare a dataframe `result` with columns: {ID,BWEIGHT} as defined in task-17\n",
    "result = pd.DataFrame()\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "#result = pd.DataFrame({'ID' : ID_judge, 'BWEIGHT' : y_pred_batch})\n",
    "\n",
    "y_pred = [item for sublist in y_pred_batch for item in sublist]\n",
    "\n",
    "result[\"BWEIGHT\"] = y_pred[:2000]\n",
    "\n",
    "\n",
    "result.insert(0, 'ID', ID_judge)\n",
    "\n",
    "\n",
    "print(result)\n",
    "result.to_csv('my_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee53a073",
   "metadata": {},
   "source": [
    "Task 18:\n",
    "[Very important] Please look back from tasks 1-17 to see either do hyperparameter tuning, data analysis, model selection to improve your initial submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9805d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID   BWEIGHT\n",
      "0        1  7.044473\n",
      "1        2  7.510963\n",
      "2        3  6.468890\n",
      "3        4  7.051380\n",
      "4        5  6.712514\n",
      "...    ...       ...\n",
      "1995  1996  5.756322\n",
      "1996  1997  7.298932\n",
      "1997  1998  7.761803\n",
      "1998  1999  7.314196\n",
      "1999  2000  5.715069\n",
      "\n",
      "[2000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#your task is to re-assign proper values based on the task requirement below.\n",
    "# YOUR CODE HERE\n",
    "y_pred = []\n",
    "#alpha=0.05\n",
    "#nEpoch=1000\n",
    "#batch_size=32\n",
    "\n",
    "y_pred = linear_regression_gd_minibatch_predict(betas_minibatch_005_32,judge_scaled_std)\n",
    "\n",
    "\n",
    "result = pd.DataFrame({'ID' : ID_judge, 'BWEIGHT' : y_pred})\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0ff79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
